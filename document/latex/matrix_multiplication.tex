\doxysection{Matrix Multiplication}
\hypertarget{matrix_multiplication}{}\label{matrix_multiplication}\index{Matrix Multiplication@{Matrix Multiplication}}
We study the classic problem, {\itshape 2D matrix multiplication}. We will start with a short introduction about the problem and then discuss how to solve it parallel CPUs.\hypertarget{matrix_multiplication_MatrixMultiplicationProblem}{}\doxysubsection{\texorpdfstring{Problem Formulation}{Problem Formulation}}\label{matrix_multiplication_MatrixMultiplicationProblem}
We are multiplying two matrices, {\ttfamily \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}} ({\ttfamily MxK}) and {\ttfamily \doxylink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}} ({\ttfamily KxN}). The numbers of columns of {\ttfamily \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}} must match the number of rows of {\ttfamily \doxylink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}}. The output matrix {\ttfamily C} has the shape of {\ttfamily }(MxN) where {\ttfamily M} is the rows of {\ttfamily \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}} and {\ttfamily N} the columns of {\ttfamily \doxylink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}}. The following example multiplies a {\ttfamily 3x3} matrix with a {\ttfamily 3x2} matrix to derive a {\ttfamily 3x2} matrix.



As a general view, for each element of {\ttfamily C} we iterate a complete row of {\ttfamily \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}} and a complete column of {\ttfamily \doxylink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}}, multiplying each element and summing them.



We can implement matrix multiplication using three nested loops.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ \mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}=0;\ \mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}<\mbox{\hyperlink{test__overwrite__node_8cpp_a52037c938e3c1b126c6277da5ca689d0}{M}};\ \mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}++)\ \{}
\DoxyCodeLine{\ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}=0;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}<\mbox{\hyperlink{main-override-static_8c_a0240ac851181b84ac374872dc5434ee4}{N}};\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{test__buffer__node_8cpp_ac4cf4b2ab929bd23951a8676eeac086b}{C}}[\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}][\mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}]\ =\ 0;}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ k=0;\ k<K;\ k++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \mbox{\hyperlink{test__buffer__node_8cpp_ac4cf4b2ab929bd23951a8676eeac086b}{C}}[\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}][\mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}]\ +=\ \mbox{\hyperlink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}}[\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}][k]\ *\ \mbox{\hyperlink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}}[k][\mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}];}
\DoxyCodeLine{\ \ \ \ \}}
\DoxyCodeLine{\ \ \}}
\DoxyCodeLine{\}}

\end{DoxyCode}
\hypertarget{matrix_multiplication_MatrixMultiplicationParallelPattern}{}\doxysubsection{\texorpdfstring{Parallel Patterns}{Parallel Patterns}}\label{matrix_multiplication_MatrixMultiplicationParallelPattern}
At a fine-\/grained level, computing each element of {\ttfamily C} is independent of each other. Similarly, computing each row of {\ttfamily C} or each column of {\ttfamily C} is also independent of one another. With task parallelism, we prefer {\itshape coarse-\/grained} model to have each task perform rather large computation to amortize the overhead of creating and scheduling tasks. In this case, we avoid intensive tasks each working on only a single element. by creating a task per row of {\ttfamily C} to multiply a row of {\ttfamily \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}} by every column of {\ttfamily \doxylink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}}.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{comment}{//\ C\ =\ A\ *\ B}}
\DoxyCodeLine{\textcolor{comment}{//\ A\ is\ a\ MxK\ matrix,\ B\ is\ a\ KxN\ matrix,\ and\ C\ is\ a\ MxN\ matrix}}
\DoxyCodeLine{\textcolor{keywordtype}{void}\ \mbox{\hyperlink{external_2taskflow_2benchmarks_2matrix__multiplication_2main_8cpp_a3a744240c07a8320395e70bc269dba64}{matrix\_multiplication}}(\textcolor{keywordtype}{int}**\ \mbox{\hyperlink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}},\ \textcolor{keywordtype}{int}**\ \mbox{\hyperlink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}},\ \textcolor{keywordtype}{int}**\ \mbox{\hyperlink{test__buffer__node_8cpp_ac4cf4b2ab929bd23951a8676eeac086b}{C}},\ \textcolor{keywordtype}{int}\ \mbox{\hyperlink{test__overwrite__node_8cpp_a52037c938e3c1b126c6277da5ca689d0}{M}},\ \textcolor{keywordtype}{int}\ K,\ \textcolor{keywordtype}{int}\ \mbox{\hyperlink{main-override-static_8c_a0240ac851181b84ac374872dc5434ee4}{N}})\ \{}
\DoxyCodeLine{\ \ \mbox{\hyperlink{classtf_1_1_taskflow}{tf::Taskflow}}\ \mbox{\hyperlink{poisson_8hpp_aa56fe360bb0ae38e0082f49e394ff825}{taskflow}};}
\DoxyCodeLine{\ \ \mbox{\hyperlink{classtf_1_1_executor}{tf::Executor}}\ \mbox{\hyperlink{thread__pool_8cpp_a543e564a8407bbeac15cb2d929fec755}{executor}};}
\DoxyCodeLine{\ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ \mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}=0;\ \mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}<\mbox{\hyperlink{test__overwrite__node_8cpp_a52037c938e3c1b126c6277da5ca689d0}{M}};\ ++\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}})\ \{}
\DoxyCodeLine{\ \ \ \ \mbox{\hyperlink{poisson_8hpp_aa56fe360bb0ae38e0082f49e394ff825}{taskflow}}.emplace([\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}},\ \&]\ ()\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}=0;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}<\mbox{\hyperlink{main-override-static_8c_a0240ac851181b84ac374872dc5434ee4}{N}};\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ k=0;\ k<K;\ k++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \mbox{\hyperlink{test__buffer__node_8cpp_ac4cf4b2ab929bd23951a8676eeac086b}{C}}[\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}][\mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}]\ +=\ \mbox{\hyperlink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}}[\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}][k]\ *\ \mbox{\hyperlink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}}[k][\mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}];\ \ \textcolor{comment}{//\ inner\ product}}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}
\DoxyCodeLine{\ \ \ \ \ \ \}}
\DoxyCodeLine{\ \ \ \ \});}
\DoxyCodeLine{\ \ \}}
\DoxyCodeLine{\ \ \mbox{\hyperlink{thread__pool_8cpp_a543e564a8407bbeac15cb2d929fec755}{executor}}.\mbox{\hyperlink{classtf_1_1_executor_a519777f5783981d534e9e53b99712069}{run}}(\mbox{\hyperlink{poisson_8hpp_aa56fe360bb0ae38e0082f49e394ff825}{taskflow}}).wait();}
\DoxyCodeLine{\}}

\end{DoxyCode}


Instead of creating tasks one-\/by-\/one over a loop, you can leverage \doxylink{classtf_1_1_flow_builder_a3b132bd902331a11b04b4ad66cf8bf77}{Taskflow\+::for\+\_\+each\+\_\+index} to create a {\itshape parallel-\/for} task. \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A} parallel-\/for task spawns a subflow to perform parallel iterations over the given range.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{comment}{//\ perform\ parallel\ iterations\ on\ the\ range\ [0,\ M)\ with\ the\ step\ size\ of\ 1}}
\DoxyCodeLine{\mbox{\hyperlink{classtf_1_1_task}{tf::Task}}\ \mbox{\hyperlink{test__partitioner__whitebox_8h_a5c4cf3d37a4eee22275de22cb9619863}{task}}\ =\ \mbox{\hyperlink{poisson_8hpp_aa56fe360bb0ae38e0082f49e394ff825}{taskflow}}.for\_each\_index(0,\ \mbox{\hyperlink{test__overwrite__node_8cpp_a52037c938e3c1b126c6277da5ca689d0}{M}},\ 1,\ [\&]\ (\textcolor{keywordtype}{int}\ \mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}})\ \{}
\DoxyCodeLine{\ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}=0;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}<\mbox{\hyperlink{main-override-static_8c_a0240ac851181b84ac374872dc5434ee4}{N}};\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}++)\ \{}
\DoxyCodeLine{\ \ \ \ \textcolor{keywordflow}{for}(\textcolor{keywordtype}{int}\ k=0;\ k<K;\ k++)\ \{}
\DoxyCodeLine{\ \ \ \ \ \ \mbox{\hyperlink{test__buffer__node_8cpp_ac4cf4b2ab929bd23951a8676eeac086b}{C}}[\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}][\mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}]\ +=\ \mbox{\hyperlink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}}[\mbox{\hyperlink{_angle_axis__mimic__euler_8cpp_ab3cd915d758008bd19d0f2428fbb354a}{m}}][k]\ *\ \mbox{\hyperlink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}}[k][\mbox{\hyperlink{_bi_c_g_s_t_a_b__simple_8cpp_a5150192f625f4d7970d61169b9567f39}{n}}];}
\DoxyCodeLine{\ \ \ \ \}\ \ \ }
\DoxyCodeLine{\ \ \}\ \ \ }
\DoxyCodeLine{\});\ }

\end{DoxyCode}


Please visit \doxylink{ParallelIterations}{Parallel Iterations} for more details.\hypertarget{matrix_multiplication_MatrixMultiplicationBenchmarking}{}\doxysubsection{\texorpdfstring{Benchmarking}{Benchmarking}}\label{matrix_multiplication_MatrixMultiplicationBenchmarking}
Based on the discussion above, we compare the runtime of computing various matrix sizes of {\ttfamily \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}}, {\ttfamily \doxylink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}}, and {\ttfamily C} between a sequential CPU and parallel CPUs on a machine of 12 Intel i7-\/8700 CPUs at 3.\+2 GHz.

 \tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ \doxylink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ C   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ CPU Sequential   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ CPU Parallel    }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ \doxylink{bench__gemm_8cpp_a37a83060ac796961b44991c836f083f7}{B}   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ C   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ CPU Sequential   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ CPU Parallel    }\\\cline{1-5}
\endhead
\PBS\centering 10x10   &\PBS\centering 10x10   &\PBS\centering 10x10   &\PBS\centering 0.\+142 ms   &\PBS\centering 0.\+414 ms    \\\cline{1-5}
\PBS\centering 100x100   &\PBS\centering 100x100   &\PBS\centering 100x100   &\PBS\centering 1.\+641 ms   &\PBS\centering 0.\+733 ms    \\\cline{1-5}
\PBS\centering 1000x1000   &\PBS\centering 1000x1000   &\PBS\centering 1000x1000   &\PBS\centering 1532 ms   &\PBS\centering 504 ms    \\\cline{1-5}
\PBS\centering 2000x2000   &\PBS\centering 2000x2000   &\PBS\centering 2000x2000   &\PBS\centering 25688 ms   &\PBS\centering 4387 ms    \\\cline{1-5}
\PBS\centering 3000x3000   &\PBS\centering 3000x3000   &\PBS\centering 3000x3000   &\PBS\centering 104838 ms   &\PBS\centering 16170 ms    \\\cline{1-5}
\PBS\centering 4000x4000   &\PBS\centering 4000x4000   &\PBS\centering 4000x4000   &\PBS\centering 250133 ms   &\PBS\centering 39646 ms   \\\cline{1-5}
\end{longtabu}


The speed-\/up of parallel execution becomes clean as we increase the problem size. For example, at {\ttfamily 4000x4000}, the parallel runtime is 6.\+3 times faster than the sequential runtime. 
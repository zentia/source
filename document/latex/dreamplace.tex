\doxysection{Standard Cell Placement}
\hypertarget{dreamplace}{}\label{dreamplace}\index{Standard Cell Placement@{Standard Cell Placement}}
We applied Taskflow to solve a VLSI placement problem. The goal is to determine the physical locations of cells (logic gates) in a fixed layout region using minimal interconnect wirelength.\hypertarget{dreamplace_UseCasesDreamPlace}{}\doxysubsection{\texorpdfstring{Dream\+Place\+: GPU-\/accelerated Placement Engine}{Dream\+Place\+: GPU-\/accelerated Placement Engine}}\label{dreamplace_UseCasesDreamPlace}
Placement is an important step in the layout generation stage of a circuit design. It places each cell of synthesized netlists in a region and optimizes their interconnect. The following figure shows a placement layout of an industrial design, adaptec1.



Modern placement typically incorporates hundreds of millions of cells and takes several hours to finish. To reduce the long runtime, recent work started investigating new CPU-\/\+GPU algorithms. We consider matching-\/based hybrid CPU-\/\+GPU placement refinement algorithm developed by \href{https://github.com/limbo018/DREAMPlace}{\texttt{ DREAMPlace}}. The algorithm iterates the following\+:

\begin{DoxyItemize}
\item \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A} GPU-\/based maximal independent set algorithm to identify cell candidates \item \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A} CPU-\/based partition algorithm to cluster adjacent cells \item \doxylink{bench__gemm_8cpp_addc86e8508f14411ec98f521c520f875}{A} CPU-\/based bipartite matching algorithm to find the best permutation of cell locations.\end{DoxyItemize}
Each iteration contains overlapped CPU and GPU tasks with nested conditions to decide the convergence.

\hypertarget{dreamplace_UseCasesDreamPlaceProgrammingEffort}{}\doxysubsection{\texorpdfstring{Programming Effort}{Programming Effort}}\label{dreamplace_UseCasesDreamPlaceProgrammingEffort}
We implemented the hybrid CPU-\/\+GPU placement algorithm using Taskflow, @\+TBB, and @\+Star\+PU. The algorithm is crafted on one GPU and many CPUs. Since \doxylink{struct_t_b_b}{TBB} and Star\+PU have no support for nested conditions, we unroll their task graphs across fixed-\/length iterations found in hindsight. The figure below shows a partial taskflow of 4 cuda\+Flows, 1 conditioned cycle, and 12 static tasks for one placement iteration.

The table below lists the programming effort of each method, measured by @\+SLOCCount. Taskflow outperforms \doxylink{struct_t_b_b}{TBB} and Star\+PU in all aspects. The whole program is about 1.\+5x and 1.\+7x less complex than that of \doxylink{struct_t_b_b}{TBB} and Star\+PU, respectively.

 \tabulinesep=1mm
\begin{longtabu}spread 0pt [c]{*{5}{|X[-1]}|}
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Method   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Lines of Code   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ \doxylink{test__concurrent__vector__v2_8cpp_a36bc74cc1ee36b525959509e2a81c81e}{Number} of Tokens   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Cyclomatic Complexity   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Cost    }\\\cline{1-5}
\endfirsthead
\hline
\endfoot
\hline
\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Method   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Lines of Code   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ \doxylink{test__concurrent__vector__v2_8cpp_a36bc74cc1ee36b525959509e2a81c81e}{Number} of Tokens   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Cyclomatic Complexity   }&\PBS\centering \cellcolor{\tableheadbgcolor}\textbf{ Cost    }\\\cline{1-5}
\endhead
\PBS\centering Taskflow   &\PBS\centering 677   &\PBS\centering 4180   &\PBS\centering 53   &\PBS\centering \$15,054    \\\cline{1-5}
\PBS\centering \doxylink{struct_t_b_b}{TBB}   &\PBS\centering 1000   &\PBS\centering 6415   &\PBS\centering 78   &\PBS\centering \$21,736    \\\cline{1-5}
\PBS\centering Star\+PU   &\PBS\centering 1279   &\PBS\centering 8136   &\PBS\centering 90   &\PBS\centering \$29,686   \\\cline{1-5}
\end{longtabu}
\hypertarget{dreamplace_UseCasesDreamPlacePerformance}{}\doxysubsection{\texorpdfstring{Performance}{Performance}}\label{dreamplace_UseCasesDreamPlacePerformance}
Using 8 CPUs and 1 GPU, Taskflow is consistently faster than others across all problem sizes (placement iterations). The gap becomes clear at large problem size; at 100 iterations, Taskflow is 17\% faster than \doxylink{struct_t_b_b}{TBB} and Star\+PU. We observed similar results using other CPU numbers. Performance saturates at about 16 CPUs, primarily due to the inherent irregularity of the placement algorithm.



The memory footprint shows the benefit of our conditional tasking. We keep nearly no growth of memory when the problem size increases, whereas Star\+PU and \doxylink{struct_t_b_b}{TBB} grow linearly due to unrolled task graphs. At a vertical scale, increasing the number of CPUs bumps up the memory usage of all methods, but the growth rate of Taskflow is much slower than the others.



In terms of energy, our scheduler is very power efficient in completing the placement workload, regardless of problem sizes and CPU numbers. Beyond 16 CPUs where performance saturates, our system does not suffer from increasing power as Star\+PU, due to our adaptive task scheduling algorithm.



For irregular task graphs akin to this placement workload, resource utilization is critical to the system throughput. We corun the same program by up to nine processes that compete for 40 CPUs and 1 GPU. Corunning a placement program is very common for searching the best parameters for an algorithm. We plot the throughput using {\itshape weighted speed-\/up} across nine coruns at two problem sizes. Both Taskflow and \doxylink{struct_t_b_b}{TBB} achieve higher throughput than Star\+PU. At the largest problem size, Taskflow outperforms \doxylink{struct_t_b_b}{TBB} and Star\+PU across all coruns.

\hypertarget{dreamplace_UseCasesDreamPlaceConclusion}{}\doxysubsection{\texorpdfstring{Conclusion}{Conclusion}}\label{dreamplace_UseCasesDreamPlaceConclusion}
We have observed two significant benefits of Taskflow over existing programming systems. The first benefit is our conditional tasking. Condition tasks encode control-\/flow decisions directly in a cyclic task graph rather than unrolling it statically across iterations, saving a lot of memory usage. The second benefit is our runtime scheduler. Our scheduler is able to adapt the number of worker threads to available task parallelism at any time during the graph execution, providing improved performance, power efficiency, and system throughput.\hypertarget{dreamplace_UseCasesDreamPlaceReferences}{}\doxysubsection{\texorpdfstring{References}{References}}\label{dreamplace_UseCasesDreamPlaceReferences}
\begin{DoxyItemize}
\item Yibo Lin, Wuxi Li, Jiaqi Gu, Haoxing Ren, Brucek Khailany and David Z. Pan, "{}\href{https://ieeexplore.ieee.org/document/8982049}{\texttt{ ABCDPlace\+: Accelerated Batch-\/based Concurrent Detailed Placement on Multi-\/threaded CPUs and GPUs}},"{} {\itshape IEEE Transactions on Computer-\/aided Design of Integrated Circuits and Systems (TCAD)}, vol. 39, no. 12, pp. 5083-\/5096, Dec. 2020 \item Yibo Lin, Shounak Dhar, Wuxi Li, Haoxing Ren, Brucek Khailany and David Z. Pan, "{}\href{lin_19_01.pdf}{\texttt{ DREAMPlace\+: Deep Learning Toolkit-\/\+Enabled GPU Acceleration for Modern VLSI Placement}}"{}, {\itshape ACM/\+IEEE Design Automation Conference (DAC)}, Las Vegas, NV, Jun 2-\/6, 2019 \end{DoxyItemize}

\doxysection{Parallel Iterations}
\hypertarget{_c_u_d_a_s_t_d_for_each}{}\label{_c_u_d_a_s_t_d_for_each}\index{Parallel Iterations@{Parallel Iterations}}
Taskflow provides standard template methods for performing parallel iterations over a range of items a CUDA GPU.\hypertarget{_c_u_d_a_s_t_d_for_each_CUDASTDParallelIterationIncludeTheHeader}{}\doxysubsection{\texorpdfstring{Include the Header}{Include the Header}}\label{_c_u_d_a_s_t_d_for_each_CUDASTDParallelIterationIncludeTheHeader}
You need to include the header file, {\ttfamily taskflow/cuda/algorithm/for\+\_\+each.hpp}, for using the parallel-\/iteration algorithm.


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{preprocessor}{\#include\ <\mbox{\hyperlink{taskflow_2cuda_2algorithm_2for__each_8hpp}{taskflow/cuda/algorithm/for\_each.hpp}}>}}

\end{DoxyCode}
\hypertarget{_c_u_d_a_s_t_d_for_each_CUDASTDIndexBasedParallelFor}{}\doxysubsection{\texorpdfstring{Index-\/based Parallel Iterations}{Index-\/based Parallel Iterations}}\label{_c_u_d_a_s_t_d_for_each_CUDASTDIndexBasedParallelFor}
Index-\/based parallel-\/for performs parallel iterations over a range {\ttfamily \mbox{[}first, last)} with the given {\ttfamily step} size. The task created by \doxylink{namespacetf_a01ad7ce62fa6f42f2f2fbff3659b7884}{tf\+::cuda\+\_\+for\+\_\+each\+\_\+index} represents a kernel of parallel execution for the following loop\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{comment}{//\ positive\ step:\ first,\ first+step,\ first+2*step,\ ...}}
\DoxyCodeLine{\textcolor{keywordflow}{for}(\textcolor{keyword}{auto}\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}=first;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}<last;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}+=step)\ \{}
\DoxyCodeLine{\ \ callable(\mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}});}
\DoxyCodeLine{\}}
\DoxyCodeLine{\textcolor{comment}{//\ negative\ step:\ first,\ first-\/step,\ first-\/2*step,\ ...}}
\DoxyCodeLine{\textcolor{keywordflow}{for}(\textcolor{keyword}{auto}\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}=first;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}>last;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}+=step)\ \{}
\DoxyCodeLine{\ \ callable(\mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}});}
\DoxyCodeLine{\}}

\end{DoxyCode}


Each iteration {\ttfamily i} is independent of each other and is assigned one kernel thread to run the callable. The following example creates a kernel that assigns each entry of {\ttfamily data} to 1 over the range {\ttfamily }\mbox{[}0, 100) with step size 1.


\begin{DoxyCode}{0}
\DoxyCodeLine{\mbox{\hyperlink{namespacetf_a0c8e4b43b5822445e2316659bbd44245}{tf::cudaDefaultExecutionPolicy}}\ policy;}
\DoxyCodeLine{\textcolor{keyword}{auto}\ \mbox{\hyperlink{imgui__impl__opengl3__loader_8h_a02796e583e939f923a255e43d2c3b177}{data}}\ =\ \mbox{\hyperlink{namespacetf_ad289846c38e3f122e1315d906243fc8b}{tf::cuda\_malloc\_shared<int>}}(100);}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{comment}{//\ assigns\ each\ element\ in\ data\ to\ 1\ over\ the\ range\ [0,\ 100)\ with\ step\ size\ 1}}
\DoxyCodeLine{\mbox{\hyperlink{namespacetf_a01ad7ce62fa6f42f2f2fbff3659b7884}{tf::cuda\_for\_each\_index}}(}
\DoxyCodeLine{\ \ policy,\ 0,\ 100,\ 1,\ [\mbox{\hyperlink{imgui__impl__opengl3__loader_8h_a02796e583e939f923a255e43d2c3b177}{data}}]\ \_\_device\_\_\ (\textcolor{keywordtype}{int}\ idx)\ \{\ \mbox{\hyperlink{imgui__impl__opengl3__loader_8h_a02796e583e939f923a255e43d2c3b177}{data}}[idx]\ =\ 1;\ \}}
\DoxyCodeLine{);}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{comment}{//\ synchronize\ the\ execution}}
\DoxyCodeLine{policy.synchronize();}

\end{DoxyCode}


The parallel-\/iteration algorithm runs {\itshape asynchronously} through the stream specified in the execution policy. You need to synchronize the stream to obtain correct results.\hypertarget{_c_u_d_a_s_t_d_for_each_CUDASTDIteratorBasedParallelFor}{}\doxysubsection{\texorpdfstring{Iterator-\/based Parallel Iterations}{Iterator-\/based Parallel Iterations}}\label{_c_u_d_a_s_t_d_for_each_CUDASTDIteratorBasedParallelFor}
Iterator-\/based parallel-\/for performs parallel iterations over a range specified by two STL-\/styled iterators, {\ttfamily first} and {\ttfamily last}. The task created by \doxylink{namespacetf_a7c449cec0b93503b8280d05add35e9f4}{tf\+::cuda\+\_\+for\+\_\+each} represents a parallel execution of the following loop\+:


\begin{DoxyCode}{0}
\DoxyCodeLine{\textcolor{keywordflow}{for}(\textcolor{keyword}{auto}\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}=first;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}<last;\ \mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}}++)\ \{}
\DoxyCodeLine{\ \ callable(*\mbox{\hyperlink{_bi_c_g_s_t_a_b__step__by__step_8cpp_acb559820d9ca11295b4500f179ef6392}{i}});}
\DoxyCodeLine{\}}

\end{DoxyCode}


The two iterators, {\ttfamily first} and {\ttfamily last}, are typically two raw pointers to the first element and the next to the last element in the range in GPU memory space. The following example creates a {\ttfamily for\+\_\+each} kernel that assigns each element in {\ttfamily gpu\+\_\+data} to 1 over the range {\ttfamily \mbox{[}data, data + 1000)}.


\begin{DoxyCode}{0}
\DoxyCodeLine{\mbox{\hyperlink{namespacetf_a0c8e4b43b5822445e2316659bbd44245}{tf::cudaDefaultExecutionPolicy}}\ policy;}
\DoxyCodeLine{\textcolor{keyword}{auto}\ \mbox{\hyperlink{imgui__impl__opengl3__loader_8h_a02796e583e939f923a255e43d2c3b177}{data}}\ =\ \mbox{\hyperlink{namespacetf_ad289846c38e3f122e1315d906243fc8b}{tf::cuda\_malloc\_shared<int>}}(1000);}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{comment}{//\ assigns\ each\ element\ in\ data\ to\ 1\ over\ the\ range\ [0,\ 1000)\ with\ step\ size\ 1}}
\DoxyCodeLine{\mbox{\hyperlink{namespacetf_a7c449cec0b93503b8280d05add35e9f4}{tf::cuda\_for\_each}}(}
\DoxyCodeLine{\ \ policy,\ \mbox{\hyperlink{imgui__impl__opengl3__loader_8h_a02796e583e939f923a255e43d2c3b177}{data}},\ \mbox{\hyperlink{imgui__impl__opengl3__loader_8h_a02796e583e939f923a255e43d2c3b177}{data}}\ +\ 1000,\ []\ \_\_device\_\_\ (\textcolor{keywordtype}{int}\&\ item)\ \{\ item\ =\ 1;\ \}}
\DoxyCodeLine{);}
\DoxyCodeLine{}
\DoxyCodeLine{\textcolor{comment}{//\ synchronize\ the\ execution}}
\DoxyCodeLine{policy.synchronize();}

\end{DoxyCode}


Each iteration is independent of each other and is assigned one kernel thread to run the callable. Since the callable runs on GPU, it must be declared with a {\ttfamily \+\_\+\+\_\+device\+\_\+\+\_\+} specifier. 
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.13.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>source: GPU Tasking (cudaFlow)</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(function() { init_search(); });
/* @license-end */
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">source
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="搜索" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- 制作者 Doxygen 1.13.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){initNavTree('_g_p_u_taskingcuda_flow.html',''); initResizable(true); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">载入中...</div>
<div class="SRStatus" id="Searching">搜索中...</div>
<div class="SRStatus" id="NoMatches">未找到</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">GPU Tasking (cudaFlow)</div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>目录</h3>
<ul>
  <li class="level1">
    <a href="#GPUTaskingcudaFlowIncludeTheHeader">Include the Header</a>
  </li>
  <li class="level1">
    <a href="#WhatIsACudaGraph">What is a CUDA Graph?</a>
  </li>
  <li class="level1">
    <a href="#Create_a_cudaFlow">Create a cudaFlow</a>
  </li>
  <li class="level1">
    <a href="#Compile_a_cudaFlow_program">Compile a cudaFlow Program</a>
  </li>
  <li class="level1">
    <a href="#run_a_cudaflow_on_a_specific_gpu">Run a cudaFlow on Specific GPU</a>
  </li>
  <li class="level1">
    <a href="#GPUMemoryOperations">Create Memory Operation Tasks</a>
  </li>
  <li class="level1">
    <a href="#OffloadAcudaFlow">Offload a cudaFlow</a>
  </li>
  <li class="level1">
    <a href="#UpdateAcudaFlow">Update a cudaFlow</a>
  </li>
  <li class="level1">
    <a href="#IntegrateCudaFlowIntoTaskflow">Integrate a cudaFlow into Taskflow</a>
  </li>
</ul>
</div>
<div class="textblock"><p>Modern scientific computing typically leverages GPU-powered parallel processing cores to speed up large-scale applications. This chapter discusses how to implement CPU-GPU heterogeneous tasking algorithms with @NvidiaCUDA.</p>
<h1><a class="anchor" id="GPUTaskingcudaFlowIncludeTheHeader"></a>
Include the Header</h1>
<p>You need to include the header file, <code>taskflow/cuda/cudaflow.hpp</code>, for creating a GPU task graph using <a class="el" href="classtf_1_1cuda_flow.html" title="class to create a cudaFlow task dependency graph">tf::cudaFlow</a>.</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="cudaflow_8hpp.html">taskflow/cuda/cudaflow.hpp</a>&gt;</span></div>
<div class="ttc" id="acudaflow_8hpp_html"><div class="ttname"><a href="cudaflow_8hpp.html">cudaflow.hpp</a></div><div class="ttdoc">cudaFlow include file</div></div>
</div><!-- fragment --><h1><a class="anchor" id="WhatIsACudaGraph"></a>
What is a CUDA Graph?</h1>
<p>CUDA Graph is a new execution model that enables a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a task graph of operations, rather than a sequence of individually-launched operations. This organization allows launching multiple GPU operations through a single CPU operation and hence reduces the launching overheads, especially for kernels of short running time. The benefit of CUDA Graph can be demonstrated in the figure below:</p>
<div class="image">
<img src="images/cuda_graph_benefit.png" alt=""/>
</div>
<p>In this example, a sequence of short kernels is launched one-by-one by the CPU. The CPU launching overhead creates a significant gap in between the kernels. If we replace this sequence of kernels with a CUDA graph, initially we will need to spend a little extra time on building the graph and launching the whole graph in one go on the first occasion, but subsequent executions will be very fast, as there will be very little gap between the kernels. The difference is more pronounced when the same sequence of operations is repeated many times, for example, many training epochs in machine learning workloads. In that case, the initial costs of building and launching the graph will be amortized over the entire training iterations.</p>
<dl class="section attention"><dt>注意</dt><dd><a class="el" href="bench__gemm_8cpp.html#addc86e8508f14411ec98f521c520f875">A</a> comprehensive introduction about CUDA Graph can be referred to the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs">CUDA Graph Programming Guide</a>.</dd></dl>
<h1><a class="anchor" id="Create_a_cudaFlow"></a>
Create a cudaFlow</h1>
<p>Taskflow leverages @cudaGraph to enable concurrent CPU-GPU tasking using a task graph model called <a class="el" href="classtf_1_1cuda_flow.html" title="class to create a cudaFlow task dependency graph">tf::cudaFlow</a>. <a class="el" href="bench__gemm_8cpp.html#addc86e8508f14411ec98f521c520f875">A</a> cudaFlow manages a CUDA graph explicitly to execute dependent GPU operations in a single CPU call. The following example implements a cudaFlow that performs an saxpy (A·X Plus Y) workload:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="cudaflow_8hpp.html">taskflow/cuda/cudaflow.hpp</a>&gt;</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// saxpy (single-precision A·X Plus Y) kernel</span></div>
<div class="line">__global__ <span class="keywordtype">void</span> <a class="code hl_function" href="bench_2btl_2libs_2_b_l_a_s_2blas_8h.html#ad53e5aed16187a6bb50b8aa5f24cca8b">saxpy</a>(<span class="keywordtype">int</span> <a class="code hl_variable" href="_bi_c_g_s_t_a_b__simple_8cpp.html#a5150192f625f4d7970d61169b9567f39">n</a>, <span class="keywordtype">float</span> <a class="code hl_variable" href="_cwise__product_8cpp.html#ad2cbe4616e813eb9af81732dca777b24">a</a>, <span class="keywordtype">float</span> *<a class="code hl_variable" href="offscreen_8c.html#a5fd331c99e778f04762be6d8173eb4d2">x</a>, <span class="keywordtype">float</span> *<a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a5e247fc24ceb70d83f6ad59149b8910a">y</a>) {</div>
<div class="line">  <span class="keywordtype">int</span> <a class="code hl_variable" href="_bi_c_g_s_t_a_b__step__by__step_8cpp.html#acb559820d9ca11295b4500f179ef6392">i</a> = <a class="code hl_variable" href="cuda__common_8h.html#ab30e5f6958c9264d3c5235d463eb2b57">blockIdx</a>.x*<a class="code hl_variable" href="cuda__common_8h.html#a1a2cdcc50d7e1505a058c5832ebe5702">blockDim</a>.x + <a class="code hl_variable" href="cuda__common_8h.html#a0a680a9519f7aaccce02a4bb4c0a0b52">threadIdx</a>.x;</div>
<div class="line">  <span class="keywordflow">if</span> (<a class="code hl_variable" href="_bi_c_g_s_t_a_b__step__by__step_8cpp.html#acb559820d9ca11295b4500f179ef6392">i</a> &lt; <a class="code hl_variable" href="_bi_c_g_s_t_a_b__simple_8cpp.html#a5150192f625f4d7970d61169b9567f39">n</a>) {</div>
<div class="line">    <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a5e247fc24ceb70d83f6ad59149b8910a">y</a>[<a class="code hl_variable" href="_bi_c_g_s_t_a_b__step__by__step_8cpp.html#acb559820d9ca11295b4500f179ef6392">i</a>] = <a class="code hl_variable" href="_cwise__product_8cpp.html#ad2cbe4616e813eb9af81732dca777b24">a</a>*<a class="code hl_variable" href="offscreen_8c.html#a5fd331c99e778f04762be6d8173eb4d2">x</a>[<a class="code hl_variable" href="_bi_c_g_s_t_a_b__step__by__step_8cpp.html#acb559820d9ca11295b4500f179ef6392">i</a>] + <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a5e247fc24ceb70d83f6ad59149b8910a">y</a>[<a class="code hl_variable" href="_bi_c_g_s_t_a_b__step__by__step_8cpp.html#acb559820d9ca11295b4500f179ef6392">i</a>];</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="comment">// main function begins</span></div>
<div class="line"><span class="keywordtype">int</span> <a class="code hl_function" href="main-override-static_8c.html#ae66f6b31b5ad750f1fe042a706a4e3d4">main</a>() {</div>
<div class="line"> </div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">unsigned</span> <a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a> = 1&lt;&lt;20;                            <span class="comment">// size of the vector</span></div>
<div class="line"> </div>
<div class="line">  std::vector&lt;float&gt; hx(<a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>, 1.0f);                      <span class="comment">// x vector at host</span></div>
<div class="line">  std::vector&lt;float&gt; hy(<a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>, 2.0f);                      <span class="comment">// y vector at host</span></div>
<div class="line"> </div>
<div class="line">  <span class="keywordtype">float</span> *dx{<span class="keyword">nullptr</span>};                                  <span class="comment">// x vector at device</span></div>
<div class="line">  <span class="keywordtype">float</span> *dy{<span class="keyword">nullptr</span>};                                  <span class="comment">// y vector at device</span></div>
<div class="line"> </div>
<div class="line">  cudaMalloc(&amp;dx, <a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>*<span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div>
<div class="line">  cudaMalloc(&amp;dy, <a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>*<span class="keyword">sizeof</span>(<span class="keywordtype">float</span>));</div>
<div class="line"> </div>
<div class="line">  tf::cudaFlow cudaflow;</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// create data transfer tasks</span></div>
<div class="line">  tf::cudaTask h2d_x = cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a02a041d5dd9e1e8958eb43e09331051e">copy</a>(dx, hx.data(), <a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>).name(<span class="stringliteral">&quot;h2d_x&quot;</span>); </div>
<div class="line">  tf::cudaTask h2d_y = cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a02a041d5dd9e1e8958eb43e09331051e">copy</a>(dy, hy.data(), <a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>).name(<span class="stringliteral">&quot;h2d_y&quot;</span>);</div>
<div class="line">  tf::cudaTask d2h_x = cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a02a041d5dd9e1e8958eb43e09331051e">copy</a>(hx.data(), dx, <a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>).name(<span class="stringliteral">&quot;d2h_x&quot;</span>);</div>
<div class="line">  tf::cudaTask d2h_y = cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a02a041d5dd9e1e8958eb43e09331051e">copy</a>(hy.data(), dy, <a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>).name(<span class="stringliteral">&quot;d2h_y&quot;</span>);</div>
<div class="line"> </div>
<div class="line">  <span class="comment">// launch saxpy&lt;&lt;&lt;(N+255)/256, 256, 0&gt;&gt;&gt;(N, 2.0f, dx, dy)</span></div>
<div class="line">  tf::cudaTask kernel = cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a1473a15a6023fbc25e1f029f2ff84aec">kernel</a>(</div>
<div class="line">    (<a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>+255)/256, 256, 0, <a class="code hl_function" href="bench_2btl_2libs_2_b_l_a_s_2blas_8h.html#ad53e5aed16187a6bb50b8aa5f24cca8b">saxpy</a>, <a class="code hl_define" href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a>, 2.0f, dx, dy</div>
<div class="line">  ).name(<span class="stringliteral">&quot;saxpy&quot;</span>);</div>
<div class="line"> </div>
<div class="line">  kernel.<a class="code hl_function" href="classtf_1_1cuda_task.html#a4a9ca1a34bac47e4c9b04eb4fb2f7775">succeed</a>(h2d_x, h2d_y)</div>
<div class="line">        .<a class="code hl_function" href="classtf_1_1cuda_task.html#abdd68287ec4dff4216af34d1db44d1b4">precede</a>(d2h_x, d2h_y);</div>
<div class="line"> </div>
<div class="line">  <span class="comment">// run the cudaflow through a stream</span></div>
<div class="line">  <a class="code hl_typedef" href="namespacetf.html#af19c9b301dc0b0fe2a51a960fa427e83">tf::cudaStream</a> stream;</div>
<div class="line">  cudaflow.run(stream)</div>
<div class="line">  stream.<a class="code hl_function" href="classtf_1_1cuda_stream_base.html#a08857ff2874cd5378e578822e2e96dd0">synchronize</a>();</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// dump the cudaflow</span></div>
<div class="line">  cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#abd73a9268b80e74803f241ee10a842b6">dump</a>(std::cout);</div>
<div class="line">}</div>
<div class="ttc" id="a_bi_c_g_s_t_a_b__simple_8cpp_html_a5150192f625f4d7970d61169b9567f39"><div class="ttname"><a href="_bi_c_g_s_t_a_b__simple_8cpp.html#a5150192f625f4d7970d61169b9567f39">n</a></div><div class="ttdeci">int n</div><div class="ttdef"><b>定义</b> BiCGSTAB_simple.cpp:1</div></div>
<div class="ttc" id="a_bi_c_g_s_t_a_b__step__by__step_8cpp_html_acb559820d9ca11295b4500f179ef6392"><div class="ttname"><a href="_bi_c_g_s_t_a_b__step__by__step_8cpp.html#acb559820d9ca11295b4500f179ef6392">i</a></div><div class="ttdeci">int i</div><div class="ttdef"><b>定义</b> BiCGSTAB_step_by_step.cpp:9</div></div>
<div class="ttc" id="a_cwise__product_8cpp_html_ad2cbe4616e813eb9af81732dca777b24"><div class="ttname"><a href="_cwise__product_8cpp.html#ad2cbe4616e813eb9af81732dca777b24">a</a></div><div class="ttdeci">Array33i a</div><div class="ttdef"><b>定义</b> Cwise_product.cpp:1</div></div>
<div class="ttc" id="abench_2btl_2libs_2_b_l_a_s_2blas_8h_html_ad53e5aed16187a6bb50b8aa5f24cca8b"><div class="ttname"><a href="bench_2btl_2libs_2_b_l_a_s_2blas_8h.html#ad53e5aed16187a6bb50b8aa5f24cca8b">saxpy</a></div><div class="ttdeci">int BLASFUNC saxpy(int *, float *, float *, int *, float *, int *)</div></div>
<div class="ttc" id="aclasstf_1_1cuda_graph_base_html_a02a041d5dd9e1e8958eb43e09331051e"><div class="ttname"><a href="classtf_1_1cuda_graph_base.html#a02a041d5dd9e1e8958eb43e09331051e">tf::cudaGraphBase::copy</a></div><div class="ttdeci">cudaTask copy(T *tgt, const T *src, size_t num)</div><div class="ttdoc">creates a memcopy task that copies typed data</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:896</div></div>
<div class="ttc" id="aclasstf_1_1cuda_graph_base_html_a1473a15a6023fbc25e1f029f2ff84aec"><div class="ttname"><a href="classtf_1_1cuda_graph_base.html#a1473a15a6023fbc25e1f029f2ff84aec">tf::cudaGraphBase::kernel</a></div><div class="ttdeci">cudaTask kernel(dim3 g, dim3 b, size_t s, F f, ArgsT... args)</div><div class="ttdoc">creates a kernel task</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:831</div></div>
<div class="ttc" id="aclasstf_1_1cuda_graph_base_html_abd73a9268b80e74803f241ee10a842b6"><div class="ttname"><a href="classtf_1_1cuda_graph_base.html#abd73a9268b80e74803f241ee10a842b6">tf::cudaGraphBase::dump</a></div><div class="ttdeci">void dump(std::ostream &amp;os)</div><div class="ttdoc">dumps the CUDA graph to a DOT format through the given output stream</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:776</div></div>
<div class="ttc" id="aclasstf_1_1cuda_stream_base_html_a08857ff2874cd5378e578822e2e96dd0"><div class="ttname"><a href="classtf_1_1cuda_stream_base.html#a08857ff2874cd5378e578822e2e96dd0">tf::cudaStreamBase::synchronize</a></div><div class="ttdeci">void synchronize() const</div><div class="ttdoc">synchronizes the associated stream</div><div class="ttdef"><b>定义</b> cuda_stream.hpp:212</div></div>
<div class="ttc" id="aclasstf_1_1cuda_task_html_a4a9ca1a34bac47e4c9b04eb4fb2f7775"><div class="ttname"><a href="classtf_1_1cuda_task.html#a4a9ca1a34bac47e4c9b04eb4fb2f7775">tf::cudaTask::succeed</a></div><div class="ttdeci">cudaTask &amp; succeed(Ts &amp;&amp;... tasks)</div><div class="ttdoc">adds precedence links from other tasks to this</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:368</div></div>
<div class="ttc" id="aclasstf_1_1cuda_task_html_abdd68287ec4dff4216af34d1db44d1b4"><div class="ttname"><a href="classtf_1_1cuda_task.html#abdd68287ec4dff4216af34d1db44d1b4">tf::cudaTask::precede</a></div><div class="ttdeci">cudaTask &amp; precede(Ts &amp;&amp;... tasks)</div><div class="ttdoc">adds precedence links from this to other tasks</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:357</div></div>
<div class="ttc" id="acuda__common_8h_html_a0a680a9519f7aaccce02a4bb4c0a0b52"><div class="ttname"><a href="cuda__common_8h.html#a0a680a9519f7aaccce02a4bb4c0a0b52">threadIdx</a></div><div class="ttdeci">dim3 threadIdx</div><div class="ttdef"><b>定义</b> cuda_common.h:11</div></div>
<div class="ttc" id="acuda__common_8h_html_a1a2cdcc50d7e1505a058c5832ebe5702"><div class="ttname"><a href="cuda__common_8h.html#a1a2cdcc50d7e1505a058c5832ebe5702">blockDim</a></div><div class="ttdeci">dim3 blockDim</div><div class="ttdef"><b>定义</b> cuda_common.h:11</div></div>
<div class="ttc" id="acuda__common_8h_html_ab30e5f6958c9264d3c5235d463eb2b57"><div class="ttname"><a href="cuda__common_8h.html#ab30e5f6958c9264d3c5235d463eb2b57">blockIdx</a></div><div class="ttdeci">dim3 blockIdx</div><div class="ttdef"><b>定义</b> cuda_common.h:11</div></div>
<div class="ttc" id="aimgui__impl__opengl3__loader_8h_html_a5e247fc24ceb70d83f6ad59149b8910a"><div class="ttname"><a href="imgui__impl__opengl3__loader_8h.html#a5e247fc24ceb70d83f6ad59149b8910a">y</a></div><div class="ttdeci">GLint y</div><div class="ttdef"><b>定义</b> imgui_impl_opengl3_loader.h:188</div></div>
<div class="ttc" id="amain-override-static_8c_html_a0240ac851181b84ac374872dc5434ee4"><div class="ttname"><a href="main-override-static_8c.html#a0240ac851181b84ac374872dc5434ee4">N</a></div><div class="ttdeci">#define N</div><div class="ttdef"><b>定义</b> main-override-static.c:141</div></div>
<div class="ttc" id="amain-override-static_8c_html_ae66f6b31b5ad750f1fe042a706a4e3d4"><div class="ttname"><a href="main-override-static_8c.html#ae66f6b31b5ad750f1fe042a706a4e3d4">main</a></div><div class="ttdeci">int main()</div><div class="ttdef"><b>定义</b> main-override-static.c:32</div></div>
<div class="ttc" id="anamespacetf_html_af19c9b301dc0b0fe2a51a960fa427e83"><div class="ttname"><a href="namespacetf.html#af19c9b301dc0b0fe2a51a960fa427e83">tf::cudaStream</a></div><div class="ttdeci">cudaStreamBase&lt; cudaStreamCreator, cudaStreamDeleter &gt; cudaStream</div><div class="ttdoc">default smart pointer type to manage a cudaStream_t object with unique ownership</div><div class="ttdef"><b>定义</b> cuda_stream.hpp:299</div></div>
<div class="ttc" id="aoffscreen_8c_html_a5fd331c99e778f04762be6d8173eb4d2"><div class="ttname"><a href="offscreen_8c.html#a5fd331c99e778f04762be6d8173eb4d2">x</a></div><div class="ttdeci">float x</div><div class="ttdef"><b>定义</b> offscreen.c:41</div></div>
</div><!-- fragment --><p>The cudaFlow graph consists of two CPU-to-GPU data copies (<code>h2d_x</code> and <code>h2d_y</code>), one kernel (<code>saxpy</code>), and two GPU-to-CPU data copies (<code>d2h_x</code> and <code>d2h_y</code>), in this order of their task dependencies.</p>
<p>We do not expend yet another effort on simplifying kernel programming but focus on tasking CUDA operations and their dependencies. In other words, <a class="el" href="classtf_1_1cuda_flow.html" title="class to create a cudaFlow task dependency graph">tf::cudaFlow</a> is a lightweight C++ abstraction over CUDA Graph. This organization lets users fully take advantage of CUDA features that are commensurate with their domain knowledge, while leaving difficult task parallelism details to Taskflow.</p>
<h1><a class="anchor" id="Compile_a_cudaFlow_program"></a>
Compile a cudaFlow Program</h1>
<p>Use @nvcc to compile a cudaFlow program:</p>
<div class="fragment"><div class="line">{.shell-session} </div>
<div class="line">~$ nvcc -<a class="code hl_namespace" href="namespacestd.html">std</a>=<a class="code hl_variable" href="bench_vec_add_8cpp.html#a41689956983587b085f9da3e48f31d99">c</a>++17 my_cudaflow.cu -I <a class="code hl_variable" href="ittnotify__static_8h.html#a7016119bc831a22d1a351d56128518ed">path</a>/to/include/<a class="code hl_function" href="poisson_8hpp.html#aa56fe360bb0ae38e0082f49e394ff825">taskflow</a> -O2 -o my_cudaflow</div>
<div class="line">~$ ./my_cudaflow</div>
<div class="ttc" id="abench_vec_add_8cpp_html_a41689956983587b085f9da3e48f31d99"><div class="ttname"><a href="bench_vec_add_8cpp.html#a41689956983587b085f9da3e48f31d99">c</a></div><div class="ttdeci">Scalar Scalar * c</div><div class="ttdef"><b>定义</b> benchVecAdd.cpp:17</div></div>
<div class="ttc" id="aittnotify__static_8h_html_a7016119bc831a22d1a351d56128518ed"><div class="ttname"><a href="ittnotify__static_8h.html#a7016119bc831a22d1a351d56128518ed">path</a></div><div class="ttdeci">void const char const char int ITT_FORMAT __itt_group_sync x void const char ITT_FORMAT __itt_group_sync s void ITT_FORMAT __itt_group_sync p void ITT_FORMAT p void ITT_FORMAT p no args __itt_suppress_mode_t unsigned int void size_t ITT_FORMAT d void ITT_FORMAT p void ITT_FORMAT p __itt_model_site __itt_model_site_instance ITT_FORMAT p __itt_model_task __itt_model_task_instance ITT_FORMAT p void ITT_FORMAT p void ITT_FORMAT p void size_t ITT_FORMAT d void ITT_FORMAT p const wchar_t ITT_FORMAT s const char ITT_FORMAT s const char ITT_FORMAT s const char ITT_FORMAT s no args void ITT_FORMAT p size_t ITT_FORMAT d no args const wchar_t const wchar_t ITT_FORMAT s __itt_heap_function void size_t int ITT_FORMAT d __itt_heap_function void ITT_FORMAT p __itt_heap_function void void size_t int ITT_FORMAT d no args no args unsigned int ITT_FORMAT u const __itt_domain __itt_id ITT_FORMAT lu const __itt_domain __itt_id __itt_id __itt_string_handle ITT_FORMAT p const __itt_domain __itt_id ITT_FORMAT p const __itt_domain __itt_id __itt_timestamp __itt_timestamp ITT_FORMAT lu const __itt_domain __itt_id __itt_id __itt_string_handle ITT_FORMAT p const __itt_domain ITT_FORMAT p const __itt_domain __itt_string_handle unsigned long long ITT_FORMAT lu const __itt_domain __itt_string_handle unsigned long long ITT_FORMAT lu const __itt_domain __itt_id __itt_string_handle __itt_metadata_type size_t void ITT_FORMAT p const __itt_domain __itt_id __itt_string_handle const wchar_t size_t ITT_FORMAT lu const __itt_domain __itt_id __itt_relation __itt_id ITT_FORMAT p const wchar_t int ITT_FORMAT __itt_group_mark d __itt_event ITT_FORMAT __itt_group_mark d void const wchar_t const wchar_t int ITT_FORMAT __itt_group_sync __itt_group_fsync x void const wchar_t int const wchar_t int int ITT_FORMAT __itt_group_sync __itt_group_fsync x void ITT_FORMAT __itt_group_sync __itt_group_fsync p void ITT_FORMAT __itt_group_sync __itt_group_fsync p void size_t ITT_FORMAT lu no args __itt_obj_prop_t __itt_obj_state_t ITT_FORMAT d const char ITT_FORMAT s const char ITT_FORMAT s __itt_frame ITT_FORMAT p __itt_counter ITT_FORMAT p __itt_counter unsigned long long ITT_FORMAT lu __itt_counter unsigned long long ITT_FORMAT lu __itt_counter __itt_clock_domain unsigned long long void ITT_FORMAT p const wchar_t ITT_FORMAT S __itt_mark_type const wchar_t ITT_FORMAT S __itt_mark_type const char ITT_FORMAT s __itt_mark_type ITT_FORMAT d __itt_caller ITT_FORMAT p __itt_caller ITT_FORMAT p no args const __itt_domain __itt_clock_domain unsigned long long __itt_id ITT_FORMAT lu const __itt_domain __itt_clock_domain unsigned long long __itt_id __itt_id void ITT_FORMAT p const __itt_domain __itt_id __itt_id __itt_string_handle ITT_FORMAT p const __itt_domain __itt_id ITT_FORMAT lu const __itt_domain __itt_clock_domain unsigned long long __itt_id __itt_string_handle __itt_scope ITT_FORMAT d const __itt_domain __itt_scope __itt_string_handle const char size_t ITT_FORMAT lu const __itt_domain __itt_clock_domain unsigned long long __itt_relation __itt_id ITT_FORMAT lu __itt_track_group __itt_string_handle __itt_track_group_type ITT_FORMAT d __itt_track ITT_FORMAT p void int const int int const char int ITT_FORMAT d void void const char * path</div><div class="ttdef"><b>定义</b> ittnotify_static.h:346</div></div>
<div class="ttc" id="anamespacestd_html"><div class="ttname"><a href="namespacestd.html">std</a></div><div class="ttdoc">Extensions to the C++ standard library.</div><div class="ttdef"><b>定义</b> array.h:631</div></div>
<div class="ttc" id="apoisson_8hpp_html_aa56fe360bb0ae38e0082f49e394ff825"><div class="ttname"><a href="poisson_8hpp.html#aa56fe360bb0ae38e0082f49e394ff825">taskflow</a></div><div class="ttdeci">void taskflow(int nx, int ny, double dx, double dy, double *f, int itold, int itnew, double *u, double *unew, int block_size, unsigned num_threads)</div><div class="ttdef"><b>定义</b> taskflow.cpp:6</div></div>
</div><!-- fragment --><p>Please visit the page <a class="el" href="_compile_taskflow_with_c_u_d_a.html">Compile Taskflow with CUDA</a> for more details.</p>
<h1><a class="anchor" id="run_a_cudaflow_on_a_specific_gpu"></a>
Run a cudaFlow on Specific GPU</h1>
<p>By default, a cudaFlow runs on the current GPU context associated with the caller, which is typically GPU <code>0</code>. Each CUDA GPU has an integer identifier in the range of <code>[0, N)</code> to represent the context of that GPU, where <code>N</code> is the number of GPUs in the system. You can run a cudaFlow on a specific GPU by switching the context to a different GPU using <a class="el" href="classtf_1_1cuda_scoped_device.html" title="class to create an RAII-styled context switch">tf::cudaScopedDevice</a>. The code below creates a cudaFlow and runs it on GPU <code>2</code>.</p>
<div class="fragment"><div class="line">{</div>
<div class="line">  <span class="comment">// create an RAII-styled switcher to the context of GPU 2</span></div>
<div class="line">  <a class="code hl_class" href="classtf_1_1cuda_scoped_device.html">tf::cudaScopedDevice</a> <a class="code hl_class" href="classcontext.html">context</a>(2);</div>
<div class="line"> </div>
<div class="line">  <span class="comment">// create a cudaFlow capturer under GPU 2</span></div>
<div class="line">  <a class="code hl_class" href="classtf_1_1cuda_flow_capturer.html">tf::cudaFlowCapturer</a> capturer;</div>
<div class="line">  <span class="comment">// ...</span></div>
<div class="line"> </div>
<div class="line">  <span class="comment">// create a stream under GPU 2 and offload the capturer to that GPU</span></div>
<div class="line">  <a class="code hl_typedef" href="namespacetf.html#af19c9b301dc0b0fe2a51a960fa427e83">tf::cudaStream</a> stream;</div>
<div class="line">  capturer.<a class="code hl_function" href="classtf_1_1cuda_flow_capturer.html#a952596fd7c46acee4c2459d8fe39da28">run</a>(stream);</div>
<div class="line">  stream.<a class="code hl_function" href="classtf_1_1cuda_stream_base.html#a08857ff2874cd5378e578822e2e96dd0">synchronize</a>();</div>
<div class="line">}</div>
<div class="ttc" id="aclasscontext_html"><div class="ttname"><a href="classcontext.html">context</a></div><div class="ttdef"><b>定义</b> base.h:2629</div></div>
<div class="ttc" id="aclasstf_1_1cuda_flow_capturer_html"><div class="ttname"><a href="classtf_1_1cuda_flow_capturer.html">tf::cudaFlowCapturer</a></div><div class="ttdoc">class to create a cudaFlow graph using stream capture</div><div class="ttdef"><b>定义</b> cuda_capturer.hpp:57</div></div>
<div class="ttc" id="aclasstf_1_1cuda_flow_capturer_html_a952596fd7c46acee4c2459d8fe39da28"><div class="ttname"><a href="classtf_1_1cuda_flow_capturer.html#a952596fd7c46acee4c2459d8fe39da28">tf::cudaFlowCapturer::run</a></div><div class="ttdeci">void run(cudaStream_t stream)</div><div class="ttdoc">offloads the cudaFlowCapturer onto a GPU asynchronously via a stream</div></div>
<div class="ttc" id="aclasstf_1_1cuda_scoped_device_html"><div class="ttname"><a href="classtf_1_1cuda_scoped_device.html">tf::cudaScopedDevice</a></div><div class="ttdoc">class to create an RAII-styled context switch</div><div class="ttdef"><b>定义</b> cuda_device.hpp:293</div></div>
</div><!-- fragment --><p><a class="el" href="classtf_1_1cuda_scoped_device.html" title="class to create an RAII-styled context switch">tf::cudaScopedDevice</a> is an RAII-styled wrapper to perform <em>scoped</em> switch to the given GPU context. When the scope is destroyed, it switches back to the original context.</p>
<dl class="section attention"><dt>注意</dt><dd><a class="el" href="classtf_1_1cuda_scoped_device.html" title="class to create an RAII-styled context switch">tf::cudaScopedDevice</a> allows you to place a cudaFlow on a particular GPU device, but it is your responsibility to ensure correct memory access. For example, you may not allocate a memory block on GPU <code>2</code> while accessing it from a kernel on GPU <code>0</code>. An easy practice for multi-GPU programming is to allocate <em>unified shared memory</em> using <code>cudaMallocManaged</code> and let the CUDA runtime perform automatic memory migration between GPUs.</dd></dl>
<h1><a class="anchor" id="GPUMemoryOperations"></a>
Create Memory Operation Tasks</h1>
<p>cudaFlow provides a set of methods for users to manipulate device memory. There are two categories, <em>raw</em> data and <em>typed</em> data. Raw data operations are methods with prefix <code>mem</code>, such as <code>memcpy</code> and <code>memset</code>, that operate in <em>bytes</em>. Typed data operations such as <code>copy</code>, <code>fill</code>, and <code>zero</code>, take <em>logical count</em> of elements. For instance, the following three methods have the same result of zeroing <code>sizeof(int)*count</code> bytes of the device memory area pointed to by <code>target</code>.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span>* target;</div>
<div class="line">cudaMalloc(&amp;target, <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a78e23769680a273f948d4bd3e946fcae">count</a>*<span class="keyword">sizeof</span>(<span class="keywordtype">int</span>));</div>
<div class="line"> </div>
<div class="line"><a class="code hl_class" href="classtf_1_1cuda_flow.html">tf::cudaFlow</a> cudaflow;</div>
<div class="line">memset_target = cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a10196f49de261a4042de328aab2452c8">memset</a>(target, 0, <span class="keyword">sizeof</span>(<span class="keywordtype">int</span>) * <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a78e23769680a273f948d4bd3e946fcae">count</a>);</div>
<div class="line">same_as_above = cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a32634c5645c14b99ceeaafe77ea5ea62">fill</a>(target, 0, <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a78e23769680a273f948d4bd3e946fcae">count</a>);</div>
<div class="line">same_as_above_again = cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#ab45bc592a33380adf74d6f1e7690bd4c">zero</a>(target, <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a78e23769680a273f948d4bd3e946fcae">count</a>);</div>
<div class="ttc" id="aclasstf_1_1cuda_flow_html"><div class="ttname"><a href="classtf_1_1cuda_flow.html">tf::cudaFlow</a></div><div class="ttdoc">class to create a cudaFlow task dependency graph</div><div class="ttdef"><b>定义</b> cudaflow.hpp:51</div></div>
<div class="ttc" id="aclasstf_1_1cuda_graph_base_html_a10196f49de261a4042de328aab2452c8"><div class="ttname"><a href="classtf_1_1cuda_graph_base.html#a10196f49de261a4042de328aab2452c8">tf::cudaGraphBase::memset</a></div><div class="ttdeci">cudaTask memset(void *dst, int v, size_t count)</div><div class="ttdoc">creates a memset task that fills untyped data with a byte value</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:911</div></div>
<div class="ttc" id="aclasstf_1_1cuda_graph_base_html_a32634c5645c14b99ceeaafe77ea5ea62"><div class="ttname"><a href="classtf_1_1cuda_graph_base.html#a32634c5645c14b99ceeaafe77ea5ea62">tf::cudaGraphBase::fill</a></div><div class="ttdeci">cudaTask fill(T *dst, T value, size_t count)</div><div class="ttdoc">creates a memset task that fills a typed memory block with a value</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:878</div></div>
<div class="ttc" id="aclasstf_1_1cuda_graph_base_html_ab45bc592a33380adf74d6f1e7690bd4c"><div class="ttname"><a href="classtf_1_1cuda_graph_base.html#ab45bc592a33380adf74d6f1e7690bd4c">tf::cudaGraphBase::zero</a></div><div class="ttdeci">cudaTask zero(T *dst, size_t count)</div><div class="ttdoc">creates a memset task that sets a typed memory block to zero</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:860</div></div>
<div class="ttc" id="aimgui__impl__opengl3__loader_8h_html_a78e23769680a273f948d4bd3e946fcae"><div class="ttname"><a href="imgui__impl__opengl3__loader_8h.html#a78e23769680a273f948d4bd3e946fcae">count</a></div><div class="ttdeci">GLsizei count</div><div class="ttdef"><b>定义</b> imgui_impl_opengl3_loader.h:226</div></div>
</div><!-- fragment --><p>The method <a class="el" href="classtf_1_1cuda_graph_base.html#a32634c5645c14b99ceeaafe77ea5ea62">tf::cudaFlow::fill</a> is a more powerful variant of <a class="el" href="classtf_1_1cuda_graph_base.html#a10196f49de261a4042de328aab2452c8">tf::cudaFlow::memset</a>. It can fill a memory area with any value of type <code>T</code>, given that <code>sizeof(T)</code> is 1, 2, or 4 bytes. The following example creates a GPU task to fill <code>count</code> elements in the array <code>target</code> with value <code>1234</code>.</p>
<div class="fragment"><div class="line">cf.fill(target, 1234, <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a78e23769680a273f948d4bd3e946fcae">count</a>);</div>
</div><!-- fragment --><p>Similar concept applies to <a class="el" href="classtf_1_1cuda_graph_base.html#a5e704c7bb669a82f4fe140ecb4576eb0">tf::cudaFlow::memcpy</a> and <a class="el" href="classtf_1_1cuda_graph_base.html#a02a041d5dd9e1e8958eb43e09331051e">tf::cudaFlow::copy</a> as well. The following two methods are equivalent to each other.</p>
<div class="fragment"><div class="line">cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a5e704c7bb669a82f4fe140ecb4576eb0">memcpy</a>(target, source, <span class="keyword">sizeof</span>(<span class="keywordtype">int</span>) * <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a78e23769680a273f948d4bd3e946fcae">count</a>);</div>
<div class="line">cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a02a041d5dd9e1e8958eb43e09331051e">copy</a>(target, source, <a class="code hl_typedef" href="imgui__impl__opengl3__loader_8h.html#a78e23769680a273f948d4bd3e946fcae">count</a>);</div>
<div class="ttc" id="aclasstf_1_1cuda_graph_base_html_a5e704c7bb669a82f4fe140ecb4576eb0"><div class="ttname"><a href="classtf_1_1cuda_graph_base.html#a5e704c7bb669a82f4fe140ecb4576eb0">tf::cudaGraphBase::memcpy</a></div><div class="ttdeci">cudaTask memcpy(void *tgt, const void *src, size_t bytes)</div><div class="ttdoc">creates a memcpy task that copies untyped data in bytes</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:926</div></div>
</div><!-- fragment --><h1><a class="anchor" id="OffloadAcudaFlow"></a>
Offload a cudaFlow</h1>
<p>To offload a cudaFlow to a GPU, you need to use tf::cudaFlow::run and pass a <a class="el" href="namespacetf.html#af19c9b301dc0b0fe2a51a960fa427e83" title="default smart pointer type to manage a cudaStream_t object with unique ownership">tf::cudaStream</a> created on that GPU. The run method is asynchronous and can be explicitly synchronized through the given stream.</p>
<div class="fragment"><div class="line"><a class="code hl_typedef" href="namespacetf.html#af19c9b301dc0b0fe2a51a960fa427e83">tf::cudaStream</a> stream;</div>
<div class="line"><span class="comment">// launch a cudaflow asynchronously through a stream</span></div>
<div class="line">cudaflow.run(stream);</div>
<div class="line"><span class="comment">// wait for the cudaflow to finish</span></div>
<div class="line">stream.<a class="code hl_function" href="classtf_1_1cuda_stream_base.html#a08857ff2874cd5378e578822e2e96dd0">synchronize</a>();</div>
</div><!-- fragment --><p>When you offload a cudaFlow using tf::cudaFlow::run, the runtime transforms that cudaFlow (i.e., application GPU task graph) into a native executable instance and submit it to the CUDA runtime for execution. There is always an one-to-one mapping between cudaFlow and its native CUDA graph representation (except those constructed by using <a class="el" href="classtf_1_1cuda_flow_capturer.html" title="class to create a cudaFlow graph using stream capture">tf::cudaFlowCapturer</a>).</p>
<h1><a class="anchor" id="UpdateAcudaFlow"></a>
Update a cudaFlow</h1>
<p>Many GPU applications require you to launch a cudaFlow multiple times and update node parameters (e.g., kernel parameters and memory addresses) between iterations. cudaFlow allows you to update the parameters of created tasks and run the updated cudaFlow with new parameters. Every task-creation method in <a class="el" href="classtf_1_1cuda_flow.html" title="class to create a cudaFlow task dependency graph">tf::cudaFlow</a> has an overload to update the parameters of a created task by that method.</p>
<div class="fragment"><div class="line"><a class="code hl_typedef" href="namespacetf.html#af19c9b301dc0b0fe2a51a960fa427e83">tf::cudaStream</a> stream;</div>
<div class="line"><a class="code hl_class" href="classtf_1_1cuda_flow.html">tf::cudaFlow</a> cf;</div>
<div class="line"> </div>
<div class="line"><span class="comment">// create a kernel task</span></div>
<div class="line"><a class="code hl_class" href="classtf_1_1cuda_task.html">tf::cudaTask</a> <a class="code hl_define" href="test__partitioner__whitebox_8h.html#a5c4cf3d37a4eee22275de22cb9619863">task</a> = cf.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a1473a15a6023fbc25e1f029f2ff84aec">kernel</a>(grid1, block1, shm1, kernel, kernel_args_1);</div>
<div class="line">cf.run(stream);</div>
<div class="line">stream.<a class="code hl_function" href="classtf_1_1cuda_stream_base.html#a08857ff2874cd5378e578822e2e96dd0">synchronize</a>();</div>
<div class="line"> </div>
<div class="line"><span class="comment">// update the created kernel task with different parameters</span></div>
<div class="line">cf.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a1473a15a6023fbc25e1f029f2ff84aec">kernel</a>(<a class="code hl_define" href="test__partitioner__whitebox_8h.html#a5c4cf3d37a4eee22275de22cb9619863">task</a>, grid2, block2, shm2, kernel, kernel_args_2);</div>
<div class="line">cf.run(stream);</div>
<div class="line">stream.<a class="code hl_function" href="classtf_1_1cuda_stream_base.html#a08857ff2874cd5378e578822e2e96dd0">synchronize</a>();</div>
<div class="ttc" id="aclasstf_1_1cuda_task_html"><div class="ttname"><a href="classtf_1_1cuda_task.html">tf::cudaTask</a></div><div class="ttdoc">class to create a task handle of a CUDA Graph node</div><div class="ttdef"><b>定义</b> cuda_graph.hpp:265</div></div>
<div class="ttc" id="atest__partitioner__whitebox_8h_html_a5c4cf3d37a4eee22275de22cb9619863"><div class="ttname"><a href="test__partitioner__whitebox_8h.html#a5c4cf3d37a4eee22275de22cb9619863">task</a></div><div class="ttdeci">#define task</div><div class="ttdef"><b>定义</b> test_partitioner_whitebox.h:76</div></div>
</div><!-- fragment --><p>Between successive offloads (i.e., iterative executions of a cudaFlow), you can <em>ONLY</em> update task parameters, such as changing the kernel execution parameters and memory operation parameters. However, you must <em>NOT</em> change the topology of the cudaFlow, such as adding a new task or adding a new dependency. This is the limitation of CUDA Graph.</p>
<dl class="section attention"><dt>注意</dt><dd>There are a few restrictions on updating task parameters in a cudaFlow. Notably, you must <em>NOT</em> change the topology of an offloaded graph. In addition, update methods have the following limitations:<ul>
<li>kernel task<ul>
<li>The kernel function is not allowed to change. This restriction applies to all algorithm tasks that are created using lambda.</li>
</ul>
</li>
<li>memset and memcpy tasks:<ul>
<li>The CUDA device(s) to which the operand(s) was allocated/mapped cannot change</li>
<li>The source/destination memory must be allocated from the same contexts as the original source/destination memory.</li>
</ul>
</li>
</ul>
</dd></dl>
<h1><a class="anchor" id="IntegrateCudaFlowIntoTaskflow"></a>
Integrate a cudaFlow into Taskflow</h1>
<p>You can create a task to enclose a cudaFlow and run it from a worker thread. The usage of the cudaFlow remains the same except that the cudaFlow is run by a worker thread from a taskflow task. The following example runs a cudaFlow from a static task:</p>
<div class="fragment"><div class="line"><a class="code hl_class" href="classtf_1_1_executor.html">tf::Executor</a> <a class="code hl_variable" href="thread__pool_8cpp.html#a543e564a8407bbeac15cb2d929fec755">executor</a>;</div>
<div class="line"><a class="code hl_class" href="classtf_1_1_taskflow.html">tf::Taskflow</a> <a class="code hl_function" href="poisson_8hpp.html#aa56fe360bb0ae38e0082f49e394ff825">taskflow</a>;</div>
<div class="line"> </div>
<div class="line"><a class="code hl_function" href="poisson_8hpp.html#aa56fe360bb0ae38e0082f49e394ff825">taskflow</a>.emplace([](){</div>
<div class="line">  <span class="comment">// create a cudaFlow inside a static task</span></div>
<div class="line">  <a class="code hl_class" href="classtf_1_1cuda_flow.html">tf::cudaFlow</a> cudaflow;</div>
<div class="line"> </div>
<div class="line">  <span class="comment">// ... create a kernel task</span></div>
<div class="line">  cudaflow.<a class="code hl_function" href="classtf_1_1cuda_graph_base.html#a1473a15a6023fbc25e1f029f2ff84aec">kernel</a>(...);</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// run the capturer through a stream</span></div>
<div class="line">  <a class="code hl_typedef" href="namespacetf.html#af19c9b301dc0b0fe2a51a960fa427e83">tf::cudaStream</a> stream;</div>
<div class="line">  capturer.<a class="code hl_function" href="classtf_1_1cuda_flow_capturer.html#a952596fd7c46acee4c2459d8fe39da28">run</a>(stream);</div>
<div class="line">  stream.<a class="code hl_function" href="classtf_1_1cuda_stream_base.html#a08857ff2874cd5378e578822e2e96dd0">synchronize</a>();</div>
<div class="line">});</div>
<div class="ttc" id="aclasstf_1_1_executor_html"><div class="ttname"><a href="classtf_1_1_executor.html">tf::Executor</a></div><div class="ttdoc">class to create an executor for running a taskflow graph</div><div class="ttdef"><b>定义</b> executor-dl.hpp:51</div></div>
<div class="ttc" id="aclasstf_1_1_taskflow_html"><div class="ttname"><a href="classtf_1_1_taskflow.html">tf::Taskflow</a></div><div class="ttdoc">class to create a taskflow object</div><div class="ttdef"><b>定义</b> taskflow.hpp:67</div></div>
<div class="ttc" id="athread__pool_8cpp_html_a543e564a8407bbeac15cb2d929fec755"><div class="ttname"><a href="thread__pool_8cpp.html#a543e564a8407bbeac15cb2d929fec755">executor</a></div><div class="ttdeci">tf::Executor executor</div><div class="ttdef"><b>定义</b> thread_pool.cpp:6</div></div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">制作者 <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.13.2 </li>
  </ul>
</div>
</body>
</html>
